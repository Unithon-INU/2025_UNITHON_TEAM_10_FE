#import <VisionCamera/FrameProcessorPlugin.h>
#import <VisionCamera/VisionCameraProxyHolder.h>
#import <VisionCamera/Frame.h>
#import <Foundation/Foundation.h>
#import <UIKit/UIKit.h> // UIImageOrientationì„ ì‚¬ìš©í•˜ê¸° ìœ„í•¨
#import <CoreMedia/CoreMedia.h>
#import <CoreML/CoreML.h>
#import <CoreVideo/CoreVideo.h>
#import <CoreImage/CoreImage.h> // CIImage ë° CIContext
#import "Yolo11N.h"

@interface TrashSorterPlugin : FrameProcessorPlugin {
    Yolo11N *model;
    CIContext *ciContext;
}
@end

@implementation TrashSorterPlugin

#pragma mark - Debug Helpers

- (NSString *)debugLogPath {
    NSArray* paths = NSSearchPathForDirectoriesInDomains(NSDocumentDirectory, NSUserDomainMask, YES);
    NSString* documentsDirectory = [paths objectAtIndex:0];
    return [documentsDirectory stringByAppendingPathComponent:@"onnx_debug.txt"];
}

- (void)appendDebugLog:(NSString *)log {
    NSString *logPath = [self debugLogPath];
    NSFileHandle *fileHandle = [NSFileHandle fileHandleForWritingAtPath:logPath];
    if (!fileHandle) {
        [log writeToFile:logPath atomically:YES encoding:NSUTF8StringEncoding error:nil];
    } else {
        [fileHandle seekToEndOfFile];
        NSString *line = [NSString stringWithFormat:@"%@\n", log];
        [fileHandle writeData:[line dataUsingEncoding:NSUTF8StringEncoding]];
        [fileHandle closeFile];
    }
}

#pragma mark - Init

- (instancetype)initWithProxy:(VisionCameraProxyHolder*)proxy withOptions:(NSDictionary*)options {
    self = [super initWithProxy:proxy withOptions:options];
    if (self) {
        [self setupModel];
    }
    return self;
}

- (instancetype)initForTesting {
    VisionCameraProxyHolder* dummyProxy = [[VisionCameraProxyHolder alloc] init];
    self = [super initWithProxy:dummyProxy withOptions:nil];
    if (self) {
        [self setupModel];
    }
    return self;
}

#pragma mark - Model Setup

- (void)setupModel {
    NSError *error = nil;
    model = [[Yolo11N alloc] init];
    
    if (!model) {
        NSLog(@"[TrashSorterPlugin] Failed to initialize Yolo11N model: %@", error);
        return;
    }
    // CIContextëŠ” ìŠ¤ë ˆë“œ-ì„¸ì´í”„í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, ê° í”„ë ˆì„ ì²˜ë¦¬ í˜¸ì¶œë§ˆë‹¤ ìƒˆ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ê±°ë‚˜,
    // ë‹¨ì¼ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•œë‹¤ë©´ ë™ê¸°í™” ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•´ì•¼ í•˜ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœí™”ë¥¼ ìœ„í•´ ì „ì—­ìœ¼ë¡œ ìœ ì§€
    self->ciContext = [CIContext contextWithOptions:nil];
    
    NSLog(@"âœ… Yolo11N model initialized successfully");
}

#pragma mark - Frame Processing

- (id _Nullable)callback:(Frame* _Nonnull)frame withArguments:(NSDictionary* _Nullable)arguments {
    @autoreleasepool {
        CVPixelBufferRef pixelBuffer = CMSampleBufferGetImageBuffer(frame.buffer);
        if (!pixelBuffer) {
            [self appendDebugLog:@"âŒ Null pixel buffer received"];
            return nil;
        }

        // VisionCamera ë¬¸ì„œì— ë”°ë¼ Frame.orientationì„ ì‚¬ìš©í•˜ì—¬ CIImageë¥¼ ì˜¬ë°”ë¥´ê²Œ íšŒì „ì‹œí‚µë‹ˆë‹¤.
        // Frame.orientationì€ UIImageOrientation ì—´ê±°í˜• ê°’ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        CIImage *ciImage = [CIImage imageWithCVPixelBuffer:pixelBuffer];
        
        // MLKit ì˜ˆì‹œì²˜ëŸ¼ frame.orientationì„ ì‚¬ìš©í•˜ì—¬ CIImageì˜ ë°©í–¥ì„ ì„¤ì •í•©ë‹ˆë‹¤.
        // CIImageëŠ” UIImageOrientation ê°’ì„ ì´í•´í•˜ê³  ìë™ìœ¼ë¡œ ë³€í™˜ì„ ì ìš©í•©ë‹ˆë‹¤.
//        if (frame.orientation != UIImageOrientationUp) { // ê¸°ë³¸ ë°©í–¥ì´ ì•„ë‹ˆë©´ íšŒì „ ì ìš©
      
//        }

        // [ì¶”ê°€] í•­ìƒ ì„¸ë¡œë¡œ ê°€ì •í•˜ê³  -90ë„ íšŒì „
        CGAffineTransform rotateToPortrait = CGAffineTransformMakeRotation(-M_PI_2);
        ciImage = [ciImage imageByApplyingTransform:rotateToPortrait];
        
        // ì´ ì‹œì ì—ì„œ ciImage.extent.sizeëŠ” í”„ë ˆì„ì˜ 'ìœ íš¨í•œ(up-right)' ë„ˆë¹„ì™€ ë†’ì´ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
        // ì˜ˆë¥¼ ë“¤ì–´, 1920x1080 (ê°€ë¡œ) ì„¼ì„œì—ì„œ ì„¸ë¡œ ëª¨ë“œ(Portrait)ë¡œ ì°ì—ˆë‹¤ë©´,
        // ciImage.extent.size.widthëŠ” 1080ì´ ë˜ê³ , ciImage.extent.size.heightëŠ” 1920ì´ ë©ë‹ˆë‹¤.
        size_t originalWidth = ciImage.extent.size.width;
        size_t originalHeight = ciImage.extent.size.height;
        
        NSLog(@"ğŸ” Processing frame - Rotated frame: %zu x %zu (Sensor Raw: %zu x %zu), Orientation: %ld",
              originalWidth, originalHeight,
              CVPixelBufferGetWidth(pixelBuffer), CVPixelBufferGetHeight(pixelBuffer),
              (long)frame.orientation); // ë””ë²„ê¹…ì„ ìœ„í•´ orientation ê°’ ë¡œê¹…

      
        // 640x640 ëª¨ë¸ ì…ë ¥ í¬ê¸°ì— ë§ì¶° ìŠ¤ì¼€ì¼ ë° íŒ¨ë”© ê³„ì‚°
        float scale, padX, padY;
        
        // Calculate scale factor (aspect fit to 640x640)
        // ì´ì œ originalWidthì™€ originalHeightëŠ” íšŒì „ì´ ì ìš©ëœ ì‹¤ì œ ì´ë¯¸ì§€ì˜ ìœ íš¨í•œ í¬ê¸°ì…ë‹ˆë‹¤.
        float s = MIN(640.0 / originalWidth, 640.0 / originalHeight);
        scale = s;

        float scaledWidth = originalWidth * s;
        float scaledHeight = originalHeight * s;

        // Calculate padding to center the image
        padX = (640.0 - scaledWidth) / 2.0;
        padY = (640.0 - scaledHeight) / 2.0;
      
      CGAffineTransform translate = CGAffineTransformMakeTranslation(padX, padY);
      CGAffineTransform scaleTransform = CGAffineTransformMakeScale(scale, scale);
      float shiftX = 0;
      float shiftY = 640;  // ìŒìˆ˜ y ì¢Œí‘œë¥¼ ë³´ì •í•˜ê¸° ìœ„í•œ ì´ë™

      // ê¸°ì¡´ transformì— ì¶”ê°€ translate ì ìš©
      CGAffineTransform fixTransform = CGAffineTransformMakeTranslation(shiftX, shiftY);
      
      // translate í›„ scale ì ìš©
      CGAffineTransform transform = CGAffineTransformConcat(scaleTransform, translate);
      transform = CGAffineTransformConcat(transform, fixTransform);
        // í•˜ë‚˜ë¡œ ì ìš©
        ciImage = [ciImage imageByApplyingTransform:transform];
      
      NSLog(@"Transformed CIImage extent: %@", NSStringFromCGRect(ciImage.extent));

   
      
      
      CIContext *context = [CIContext contextWithOptions:nil];
      CGRect extent = [ciImage extent];
      CGImageRef cgImage = [context createCGImage:ciImage fromRect:extent];
      UIImage *uiImage = [UIImage imageWithCGImage:cgImage];
      NSData *data = UIImagePNGRepresentation(uiImage);
      
      NSArray* paths = NSSearchPathForDirectoriesInDomains(NSDocumentDirectory, NSUserDomainMask, YES);
      NSString* documentsDirectory = [paths objectAtIndex:0];
      NSString* yourFilePath =  [documentsDirectory stringByAppendingPathComponent:@"output.png"];

      [data writeToFile:yourFilePath atomically:YES];
      CGImageRelease(cgImage);
      
      


        // Destination buffer (640x640 for CoreML model input)
        CVPixelBufferRef destPixelBuffer = NULL;
        NSDictionary *attrs = @{
            (id)kCVPixelBufferCGImageCompatibilityKey: @YES,
            (id)kCVPixelBufferCGBitmapContextCompatibilityKey: @YES
        };
        
        CVReturn status = CVPixelBufferCreate(kCFAllocatorDefault,
                                              640, 640,
                                              kCVPixelFormatType_32BGRA, // ëª¨ë¸ì´ ìš”êµ¬í•˜ëŠ” í”½ì…€ í¬ë§·
                                              (__bridge CFDictionaryRef)attrs,
                                              &destPixelBuffer);
        if (status != kCVReturnSuccess) {
            NSLog(@"Failed to create CVPixelBuffer: %d", status);
            return NULL;
        }
        
        [self->ciContext render:ciImage toCVPixelBuffer:destPixelBuffer];

        float iouThreshold = 0.5;
        float confidenceThreshold = 0.5;
        if (arguments != nil) {
            if (arguments[@"iouThreshold"] != nil)
                iouThreshold = [arguments[@"iouThreshold"] floatValue];
            if (arguments[@"confidenceThreshold"] != nil)
                confidenceThreshold = [arguments[@"confidenceThreshold"] floatValue];
        }

        NSError *error = nil;
        Yolo11NInput *input = [[Yolo11NInput alloc] initWithImage:destPixelBuffer
                                                     iouThreshold:iouThreshold
                                              confidenceThreshold:confidenceThreshold];

        Yolo11NOutput *output = [model predictionFromFeatures:input error:&error];

        // âœ… í•´ì œ: destPixelBuffer (processedBuffer ì—­í• )
        CVPixelBufferRelease(destPixelBuffer);
        destPixelBuffer = NULL;

        if (error || !output) {
            [self appendDebugLog:[NSString stringWithFormat:@"âŒ Prediction failed: %@", error]];
            NSLog(@"[TrashSorterPlugin] Prediction failed: %@", error);
            return nil;
        }

        MLMultiArray *coordinates = output.coordinates;
        MLMultiArray *confidence = output.confidence;

        if (!coordinates || !confidence) {
            [self appendDebugLog:@"âŒ Invalid prediction output"];
            return nil;
        }

        NSUInteger boxCount = coordinates.shape[0].unsignedIntegerValue;
        NSUInteger classCount = confidence.shape[1].unsignedIntegerValue;
        NSMutableArray *results = [NSMutableArray array];

        for (NSUInteger i = 0; i < boxCount; i++) {
            float maxConfidence = 0;
            NSUInteger classIdx = 0;

            for (NSUInteger c = 0; c < classCount; c++) {
                NSArray<NSNumber *> *confIndices = @[@(i), @(c)];
                float conf = [confidence[confIndices] floatValue];
                if (conf > maxConfidence) {
                    maxConfidence = conf;
                    classIdx = c;
                }
            }

            if (maxConfidence >= confidenceThreshold) {
                // CoreML ëª¨ë¸ì—ì„œ ë‚˜ì˜¨ ì¢Œí‘œ (640x640 ê¸°ì¤€ 0.0~1.0 ìƒëŒ€ì¢Œí‘œë¡œ ê°€ì •)
                NSArray<NSNumber *> *xIndices = @[@(i), @(0)];
                NSArray<NSNumber *> *yIndices = @[@(i), @(1)];
                NSArray<NSNumber *> *wIndices = @[@(i), @(2)];
                NSArray<NSNumber *> *hIndices = @[@(i), @(3)];

                float xCenterNormalized = [coordinates[xIndices] floatValue]; // 0.0 ~ 1.0
                float yCenterNormalized = [coordinates[yIndices] floatValue]; // 0.0 ~ 1.0
                float widthNormalized = [coordinates[wIndices] floatValue];   // 0.0 ~ 1.0
                float heightNormalized = [coordinates[hIndices] floatValue]; // 0.0 ~ 1.0

                // 1. ëª¨ë¸ì´ ë±‰ëŠ” 0~1 ìƒëŒ€ì¢Œí‘œë¥¼ 640x640 í”½ì…€ ê¸°ì¤€ ì ˆëŒ€ ì¢Œí‘œë¡œ ë³€í™˜
                float xCenter640Pixel = xCenterNormalized * 640.0;
                float yCenter640Pixel = yCenterNormalized * 640.0;
                float width640Pixel = widthNormalized * 640.0;
                float height640Pixel = heightNormalized * 640.0;
                
                // 2. íŒ¨ë”© ì œê±° (640x640 ìº”ë²„ìŠ¤ ë‚´ì—ì„œ ì‹¤ì œ ì´ë¯¸ì§€ê°€ ìˆëŠ” ë¶€ë¶„ìœ¼ë¡œ ì¢Œí‘œ ì´ë™)
                float unpaddedXCenter = xCenter640Pixel - padX;
                float unpaddedYCenter = yCenter640Pixel - padY;
                
                // 3. ìŠ¤ì¼€ì¼ë§ ë˜ëŒë¦¬ê¸° (ì›ë³¸ ì´ë¯¸ì§€ í”½ì…€ í¬ê¸°ë¡œ ë³€í™˜)
                // ì´ì œ originalWidthì™€ originalHeightëŠ” ì´ë¯¸ íšŒì „ì´ ì ìš©ëœ ìœ íš¨í•œ ë„ˆë¹„/ë†’ì´ì…ë‹ˆë‹¤.
                // ë”°ë¼ì„œ ì¢Œí‘œë„ ê·¸ì— ë§ì¶° ë³€í™˜ë©ë‹ˆë‹¤.
                float originalPixelXCenter = unpaddedXCenter / scale;
                float originalPixelYCenter = unpaddedYCenter / scale;
                float originalPixelWidth = width640Pixel / scale;
                float originalPixelHeight = height640Pixel / scale;
                
                // 4. ì›ë³¸ í”„ë ˆì„ ë„ˆë¹„/ë†’ì´ ê¸°ì¤€ 0.0~1.0 ìƒëŒ€ ì¢Œí‘œë¡œ ë³€í™˜
                float relativeXCenter = originalPixelXCenter / originalWidth;
                float relativeYCenter = originalPixelYCenter / originalHeight;
                float relativeWidth = originalPixelWidth / originalWidth;
                float relativeHeight = originalPixelHeight / originalHeight;
                
                NSDictionary *detection =
                    @{
                        @"classId": @(classIdx),
                        @"confidence": @(maxConfidence),
                        @"box": @{
                            @"x": @(relativeXCenter),
                            @"y": @(relativeYCenter),
                            @"width": @(relativeWidth),
                            @"height": @(relativeHeight)
                        }
                    };
                [results addObject:detection];
            }
        }

        // âœ… í•´ì œ: CoreML ê°ì²´ nil ì²˜ë¦¬
        coordinates = nil;
        confidence = nil;
        output = nil;
        input = nil;

        return  @{
            @"detections": results,
            @"scale": @(scale),
            @"padX": @(padX),
            @"padY": @(padY),
            @"frameWidth": @(originalWidth), // íšŒì „ì´ ì ìš©ëœ ìœ íš¨ ë„ˆë¹„
            @"frameHeight": @(originalHeight) // íšŒì „ì´ ì ìš©ëœ ìœ íš¨ ë†’ì´
        };
    }
}

VISION_EXPORT_FRAME_PROCESSOR(TrashSorterPlugin, detect)

@end
